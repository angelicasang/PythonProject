{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afa73e2e",
   "metadata": {},
   "source": [
    "# Statistical Inference\n",
    "## Lesson overview:\n",
    "In this lesson we will go over statistical inference. We will start with one-sample tests (both for means and proportions), as well as calculating confidence intervals. Then, we will move to two-sample and paired tests. Lastly, we will also guide you to perform one-way ANOVA test, simple linear regression and multiple linear regression by using statsmodels.formula.api.\n",
    "\n",
    "By the end of this session you will be able to:\n",
    "* Use statsmodels.stats.proportion, scipy.stats, statsmodels.api and statsmodels.formula.api to perform statistical inference.\n",
    "* Practice creating plots for checking conditions.\n",
    "* Performing one-way anova, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71f38516",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Coasters_2015.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformula\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ols\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Read csv file\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m coasters \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCoasters_2015.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Coasters_2015.csv'"
     ]
    }
   ],
   "source": [
    "# Import libraries and functions\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from statsmodels.stats.proportion import proportions_ztest, proportion_confint\n",
    "from scipy.stats import t, probplot, ttest_1samp, ttest_ind, ttest_rel\n",
    "from statsmodels.api import qqplot, stats\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Read csv file\n",
    "coasters = pd.read_csv(\"Coasters_2015.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c668efd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473c99cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ad417f",
   "metadata": {},
   "source": [
    "### One-sample proportion z-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e501ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many rollercoasters have a steel track v/s how many have a wood track\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102b91f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the roportion of steel rollercoasters\n",
    "steel_p = 212 / (212+29)\n",
    "\n",
    "# Print the proportion\n",
    "print(\"The proportion of steel coasters in 2015 is equal to: \", steel_p)             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a98924",
   "metadata": {},
   "source": [
    "**Check conditions:**\n",
    "* n < 10% of total population. We can assume that 241 rollercoasters is less than 10% of all rollercoasters in the world.\n",
    "* random: we can assume that we were given a random sample of rollercoasters.\n",
    "* successes vs failures : 241 * 0.88 = 212, 241 * 0.12 = 29. Both values are greater than 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4da303d",
   "metadata": {},
   "source": [
    "However, a rollercoasters' expert suggests that during 2022 the proportion of steel rollercoasters has changed. Her study shows that 320 out of 355 coasters have a steel track."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3917590c",
   "metadata": {},
   "source": [
    "* h0 : p̂ = steel_p -> the proportion of steel coasters in 2022 is 0.88\n",
    "* ha : p̂ ≠ steel_p -> the proportion of steel coasters in 2022 is different from 0.88"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f32799",
   "metadata": {},
   "source": [
    "To test our null hypothesis, we will make use of proportions_ztest which we imported at the beginning of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb16cb8",
   "metadata": {},
   "source": [
    "**Arguments**\n",
    "* count = # ; number of successes if Null Hypothesis is True. (total observed * hypothesized proportion)\n",
    "* nobs = # ; size of observed sample\n",
    "* value = # ; observed proportion\n",
    "* alternative = \"nameOfAlternative\" ; Type of test(two-sided or one-sided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda58ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do your math here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64475593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store z_test and p_value\n",
    "z_test , p_value = proportions_ztest(count =  , nobs = , value = , alternative = \"two-sided\")\n",
    "\n",
    "# Print results\n",
    "print(\"The z test values is: \", z_test, \". The p-value is: \", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eb6a5a",
   "metadata": {},
   "source": [
    "Since the p-value is greater than 0.05 we fail to reject the null hypothesis. We do not have enough evidence to say that the proportion of steel rollercoasters in 2022 has changed sinced 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b2c4bb",
   "metadata": {},
   "source": [
    "### Confidence Interval for proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a2b74a",
   "metadata": {},
   "source": [
    "Now let's calculate our confidence interval for steel track proportion using our 2015 data. In order to do so, we will use proportion_confint which we imported at the beginning of the notebook.\n",
    "\n",
    "**Arguments:**\n",
    "* count = # ; number of successes\n",
    "* nobs = # ; size of observed sample\n",
    "* alpha = # ; the desired significance level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d70cf70",
   "metadata": {},
   "source": [
    "**Check conditions:**\n",
    "* n < 10% of total population. We can assume that 241 rollercoasters is less than 10% of all rollercoasters in the world.\n",
    "* random: we can assume that we were given a random sample of rollercoasters.\n",
    "* successes vs failures : 241 * 0.88 = 212, 241 * 0.12 = 29. Both values are greater than 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5132f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caclulate confidence interval and store in conf_prop\n",
    "conf_prop = proportion_confint(count = , nobs = , alpha = )\n",
    "print(conf_prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70913665",
   "metadata": {},
   "source": [
    "We are 95% confident that between 83% and 92% of the roller coasters in the world have a steel track."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4573110",
   "metadata": {},
   "source": [
    "### Confidence Interval for means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1a0dac",
   "metadata": {},
   "source": [
    "Now let's calculate our confidence interval for averaged speed using our 2015 data. In order to do so, we will use t.interval which we imported at the beginning of the notebook.\n",
    "\n",
    "**Check conditions:**\n",
    "* n >= 30 , 241 > 30\n",
    "* independence: we can assume it is a random sample, and that the average speed of one rollecoaster does not affect the rest.\n",
    "\n",
    "**Arguments:**\n",
    "* loc = # ; mean value from our sample\n",
    "* scale = # ; standard deviation from our sample\n",
    "* df = # ; degrees of freedom (n-1)\n",
    "* alpha = desired significance value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1ae867",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate standard deviation for speed\n",
    "sd_speed =\n",
    "\n",
    "# Calculate mean for speed\n",
    "mean_speed = \n",
    "\n",
    "# Assign the t-interval values into t_speed\n",
    "t_speed = t.interval(loc = , scale = ,df = , alpha = )\n",
    "\n",
    "# Print our results\n",
    "print(\"The t-interval is: \", t_speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebe2a26",
   "metadata": {},
   "source": [
    "We are 95% confident that the average speed of rollercoasters in the world will be between 54 and 56 mph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638fda9d",
   "metadata": {},
   "source": [
    "## Parametric Tests\n",
    "To practice performing statistical inference in Python, we begin by importing a 2015 dataset that contains information on rollercoasters in international amusement parks. Take a glimpse at the data using the *head( )* , *tail( )*, or *describe( )* functions to familiarize yourself with the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f796ee44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import data \n",
    "\n",
    "\n",
    "# Glimpse data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1511f983",
   "metadata": {},
   "source": [
    "## One-sample t-test\n",
    "\n",
    "We will begin by performing a one-sample t-test. Recall that the purpose of this parametric test is to determine if there is a significant difference between a sample mean and the hypothesized population mean. Let's say we were interested in learning the true average speed of roller coasters in this dataset. Let's begin by finding the mean of our data's speed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70540ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find mean\n",
    "\n",
    "\n",
    "# Print the result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fcfb20",
   "metadata": {},
   "source": [
    "Roller coaster expert, Biddy Martin, previously stated that roller coasters actually travel at an average speed of 52 mph. Lets put her word to the test and perform a one-sample t-test to determine if this is likely to be true based on our sample. \n",
    "+ H0: The average speed of roller coasters is 52 mph.\n",
    "+ HA: The average speed of roller coasters is not 52 mph.\n",
    "\n",
    "Before we can put this mean to the test, let's follow our typical inference procedures and check the conditions. For a one-sample t-test we need to check the independence assumption and the normal population assumption. Our data satisifies the independence assumption because the data arises from a random sample of roller coasters.\n",
    "\n",
    "To check the normal population assumption, we need to produce a histogram of the variable in question or a Normal probability plot. Since you have learned in previous lessons how to create a histogram, we will use this as an opportunity to teach you how to create a Normal probability plot, aka a Q-Q plot. \n",
    "\n",
    "The method we will use to specify create the Q-Q plot is from the *scipy.stats* plot, called *probplot( )*. We will need to specify the following options within the parentheses of this method to generate our desired plot:\n",
    "+ data = our dataset and the variable in question, e.g. dataset_name[\"variable_name\"]\n",
    "+ plot = pyplot, this instructs Python to plot the quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394d7314",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Draw the Q-Q plot\n",
    "\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49be5122",
   "metadata": {},
   "source": [
    "In the Normal probability plot above, we observe some deviation from the best fit line in the upper tail. However, given that this is a rather large sample (n > 50), we are safe to use *t* methods since the data is not extremely skewed. We will, however, still proceed with caution.\n",
    "\n",
    "Using the function *ttest_1samp( )* from the *scipy.stats* library, let's perform our one-sample t-test. Within this new function, we will need to supply information for the following options:\n",
    "+ data = our dataset and the variable in question, e.g. dataset_name[\"variable_name\"]\n",
    "+ popmean = expected value in null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f689383f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assign the results of our test to two variables\n",
    "\n",
    "\n",
    "# View the results\n",
    "print(\"P Value:\", p_value)\n",
    "print(\"t-test Value:\", t_test_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cc1c6e",
   "metadata": {},
   "source": [
    "Great work! We have just performed our first one-sample t-test. Now lets instruct Python to determine for us whether or not the P Value for our t-test is small enough to reject the null hypothesis that the average speed is 52 mph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138e61b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use 0.05 or 5% as the significance level of alpha.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9bef90",
   "metadata": {},
   "source": [
    "Unfortunately, it appears that Biddy may have been wrong, as we have to reject the null hypothesis that the average speed of a roller coaster is 52 mph at a significant level of 0.05. What a shocker! Biddy usually gets this stuff right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3c15dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: We do not need to assign the output of the function ttest_1samp to two values.\n",
    "# Python automatically outputs our t-test value and p-value.\n",
    "# However, it is good practice to assign these values so we can reuse them later.\n",
    "\n",
    "ttest_1samp(Coasters[\"Speed\"], 52)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380316a7",
   "metadata": {},
   "source": [
    "## Two-sample t-test\n",
    "\n",
    "Recall that a two-sample t-test is used to determine if there is a significant difference between the sample means of two independent groups. This statistical inference method is also know as the independent samples t-test. Using the same dataset as before, lets see if the mean speed is the same for roller coasters made of wood and steel.\n",
    "+ H0: The mean speeds for steel roller coasters and wood coasters are equal.\n",
    "+ HA: The mean speeds are different.\n",
    "\n",
    "As usual, we will start off by adding the libraries and functions we will need to use. Then, we will need to create two new dataframes from our original dataset, because the method for a two-sample t-test requires two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f419958c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Separates the data into two new dataframes, where one is only Steel rollercoasters and the other is only Wood\n",
    "steel = Coasters[Coasters.Track == \"Steel\"]\n",
    "wood = Coasters[Coasters.Track == \"Wood\"]\n",
    "\n",
    "# Confirm this for yourself by looking at the first few rows of each dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3726349",
   "metadata": {},
   "source": [
    "Now that we have completed our set-up, let's check the conditions for inference. For a two-sample t-test, we need to check the randomization condition, independent groups condition, and the nearly normal condition. Again, since this is a random sample of roller coasters, we can satisfy the randomization assumption. Independence is satisified because one roller coaster being made of wood does not impact another roller coaster's likelihood to be made of steel. To test the normality condition, let's produce two Q-Q plots: one for each track type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd941efc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Draw the Q-Q plot\n",
    "\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc042122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the Q-Q plot\n",
    "\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ae759d",
   "metadata": {},
   "source": [
    "The Q-Q plot for steel rollercoasters satisfies the normality condition, even through there is some slight deviation in the upper tail. Additionally, the sample size is large enough so that this deviation would not be a major issue. The Q-Q plot for wood rollercoasters visualizes substantially less roller coasters that are made from wood, but none of the roller coasters deviate dramatically from the line of best fit. Given these observations, we can proceed to complete our statistical inference.\n",
    "\n",
    "Using the function *ttest_ind( )* from the scipy.stats library, let's perform our one-sample t-test. Within this new function, we will need to supply information for the following options:\n",
    "\n",
    "+ data = our first dataset and the variable in question, e.g. dataset_name[\"variable_name\"]\n",
    "+ data = our second dataset and the variable in question, e.g. dataset_name[\"variable_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c761c7e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assign the results of our test to two variables\n",
    "# IMPORTANT, \"ttest_ind\" requires you to specify a variable within your dataset\n",
    "\n",
    "\n",
    "# Display the results\n",
    "print(\"p-values:\", p)\n",
    "print(\"t-test:\", stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439d10ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use 0.05 or 5% as the significance level of alpha.\n",
    "if p < 0.05:\n",
    "    \n",
    "    print(\"Reject the null hypothesis\")\n",
    "    \n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487f6cb9",
   "metadata": {},
   "source": [
    "At a significance level of 0.05, we fail to reject the null hypothesis that the speeds of the two different track types of roller coasters are the same. Let's take a look at the actual means of our data and see what we think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0560efd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find the mean speed for steel tracks\n",
    "print(np.mean(steel[\"Speed\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d89f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the mean speed for wooden tracks\n",
    "print(np.mean(wood[\"Speed\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a67c16",
   "metadata": {},
   "source": [
    "Some might argue that the difference between the mean speeds is substantial. However, according to our test, we lack sufficient evidence to reject the null hypothesis and state that speed changes depending on track type. Oh well. Maybe with a different sample dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d161b4f1",
   "metadata": {},
   "source": [
    "## Paired sample t-test\n",
    "Our next statistical inference task is to complete a paired sample t-test, or a dependent sample t-test, on our Coasters dataset. Remind yourselves that a paired sample t-test checks if the mean difference between two observations of the same group is 0. These tests are particularly useful to determine the impact of a treatment in an experiment.\n",
    "\n",
    "In this example, the data comes from Dr. Chico’s introductory statistics class. Students in the class take two tests over the course of the semester. Dr. Chico gives notoriously difficult exams with the intention of motivating her students to work hard in the class and thus learn as much as possible. Dr. Chico’s theory is that the first test will serve as a “wake up call” for her students, such that when they realize how difficult the class actually is they will be motivated to study harder and earn a higher grade on the second test than they got on the first test.\n",
    "\n",
    "To determine if Dr. Chico's \"wake up call\" truly works, let's perform a paired sample t-test to determine whether or not a student's performance on the second exam is significantly different from their performance on the first exam.\n",
    "+ H0: The average scores for each test will be the same.\n",
    "+ HA: The average scores for each test are not the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd39ad46",
   "metadata": {},
   "source": [
    "Let's begin by importing the data and taking a look at its format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2663eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data \n",
    "Chico = pd.read_csv(\"chico_wide.csv\")\n",
    "\n",
    "# Glimpse data\n",
    "Chico.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07e9982",
   "metadata": {},
   "source": [
    "Before we can begin our test, we will need to create two separate sets of data: one with the scores from the first exam and one with the scores from the second exam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0defa373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores on the first exam\n",
    "\n",
    "\n",
    "# Scores on the second exam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fbf20e",
   "metadata": {},
   "source": [
    "Recall that in a paired sample t-test, the conditions for inference are the paired data condition, independence, and normality. We know that the data satisfy the paired data condition because these two exam scores are dependent on a single student. Given that the data is paired, the groups are not independent, thus, here we are looking for pairwise differences. We can be certain that this data is independent because the observed differences are a representative sample from a population of interest. Additionally, one student's performance does not impact another student's performance on an exam. To check the normality condition, we can again produce our Q-Q plots to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a5de8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Draw the Q-Q plot\n",
    "\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b142d87b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Draw the Q-Q plot\n",
    "\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63649229",
   "metadata": {},
   "source": [
    "Seeing as all of the datapoints roughly follow the line of best fit, we can assume that our data follows the normality condition. We will proceed to test our null hypothesis that Dr. Chico's theory produces no significant difference in students' performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5329e86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add method for our paired test\n",
    "\n",
    "\n",
    "# Assign the results of our test to two variables\n",
    "\n",
    "\n",
    "# Display the results\n",
    "print(\"p-values:\", p)\n",
    "print(\"t-test:\", stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41acc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use 0.05 or 5% as the significance level of alpha.\n",
    "if p < 0.05:\n",
    "    \n",
    "    print(\"Reject the null hypothesis\")\n",
    "    \n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f76443",
   "metadata": {},
   "source": [
    "When alpha is equal to 0.05, we reject the null hypothesis that Dr. Chico's theory produces no significant difference in students' performance. Rather, it appears that students performed significantly better on the second exam, which may be a result of Dr. Chico making the first exam particularly challenging. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d94098",
   "metadata": {},
   "source": [
    "## One-way ANOVA\n",
    "\n",
    "In the case where we have two or multiple groups that we would like to compare at the same time, we will want to use ANOVA as our statistical inference test. ANOVA checks for a signficant difference between and within multiple groups, and it can test multiple hypotheses at once. \n",
    "\n",
    "In our example, we will analyze the backpack weight to body weight ratios of students in their first through third years of their undergraduate studies. \n",
    "+ H0: There is no significant difference between the ratios of these year groups.\n",
    "+ H0: There is a significant difference between the ratios of these year groups.\n",
    "\n",
    "Let's begin by familiarizing ourselves with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf1b261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data \n",
    "bp = pd.read_csv(\"Backpack.csv\")\n",
    "\n",
    "# Glimpse the data\n",
    "bp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb8ef60",
   "metadata": {},
   "source": [
    "Let's check the conditions for inference for one-way ANOVA which are linearity, independence, equal variance, and normality. Our data satisfies the independence condition because one student's backpack to bodyweight ratio does not impact another student's ratio. We will produce a Normal Q-Q plot to check normality, and now we will also teach you how to create a Residuals vs. Fitted plot to check linearity and equal variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f699a28b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Draw the Q-Q plot\n",
    "\n",
    "\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9f0534",
   "metadata": {},
   "source": [
    "Based on the probability plot above, the data looks to be pretty normal with the exception of some slight deviation in the upper and lower tails. We will proceed to create the Residuals vs. Fitted plot with caution.\n",
    "\n",
    "To produce the following plot, we will need to create the model for our one-way ANOVA. In this case, we will need to articulate to Python that we are trying to understand the relationship between backpack ratios and a student's year. After fitting the model, we will then need to direct Python to calculate the fitted and residual values. After that, we will be using matplotlib.pyplot to create a scatterplot of the residuals vs. fitted points.\n",
    "\n",
    "When fitting a linear model, we will be using a new method called *ols( )* from the *statsmodels.formul.api* library. As good practice, we will assign the output to a variable. Within the parentheses, we must specify the following options:\n",
    "+ data = our dataset\n",
    "+ formula = \"dependent_variable ~ explanatory_variable\"\n",
    "+ .fit() = function written after the parentheses that instructs Python to fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27af628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting linear model\n",
    "\n",
    "\n",
    "# Fitted values that predict the backpack ratio\n",
    "\n",
    "\n",
    "# Observed ratios - fitted values = residuals\n",
    "# .resid finds the residuals\n",
    "\n",
    "\n",
    "# Residuals vs. fitted values\n",
    "\n",
    "\n",
    "# Adds a line to more clearly see variance from 0\n",
    "\n",
    "\n",
    "# Add labels\n",
    "plt.xlabel(\"Fitted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Fitted vs. Residuals\")\n",
    "\n",
    "# Display the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c5316c",
   "metadata": {},
   "source": [
    "The fitted vs. residuals plot demonstrates that the conditions for linearity and equal variance are not satisifed by this dataset. The data points are showing clear patterns and do not fall around the zero line. For the sake of learning how to do ANOVA, we will continue without taking the results of the test to mean much.\n",
    "\n",
    "Since our data is set-up and we have checked conditions, we can begin our one-way ANOVA test to see if backpack to bodyweight ratio changes depending on a student's year. We will generate a one-way ANOVA table using the function *stats.anova_lm* from the *statsmodels.api* library. As good practice, we will assign our table to a variable, and within the parentheses, specify the following options:\n",
    "+ model = the model name assigned when we fit the model above\n",
    "+ typ = the type of ANOVA test to perform, e.g. \"1\" for one-way ANOVA, \"2\" for two-way ANOVA, and so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6cbe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ANOVA table\n",
    "\n",
    "\n",
    "# Display the ANOVA table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daba04f",
   "metadata": {},
   "source": [
    "Just now, we have tested the hypothesis that there is no difference between the mean backpack ratio for students of different years in their undergraduate studies. Our results show us that we fail to reject the null hypothesis at a significance level of 0.05, because the p-value for our F-statistic is 0.47. Meaning, there is no significant difference between the backpack to bodyweight ratios for students of different class years. However, given that our data fails the conditions for one-way ANOVA's inference, we should not take these results as fact and try again with a better dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84fe578",
   "metadata": {},
   "source": [
    "## Inference for Linear Regression using statsmodels.api\n",
    "Let's learn how to perform inference with linear regression! We will use a dataset from the world health organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caa528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data set\n",
    "who = pd.read_csv(\"world_health.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2774e782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the data set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6cf6c5",
   "metadata": {},
   "source": [
    "Is there an association between schooling and life expectancy after adjusting for relevant confounders?\n",
    "* h0 : there's no association between schooling and life expectancy\n",
    "* ha : there's an association between schooling and life expectancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8321b52c",
   "metadata": {},
   "source": [
    "To plot some of the graphs for checking conditions and asummptions, we must fit our data into our linear model. We will use ols which we imported at the beginning of the notebook.\n",
    "\n",
    "**Arguments:**\n",
    "* data = dataframe\n",
    "* formula = \"dependent ~ independent\" \n",
    "\n",
    "After writing our arguments, we will type .fit()\n",
    "\n",
    "* smf.ols(data = dataframe, formula = \"dependent ~ independent\").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaf9af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fit model\n",
    "model1 = ols(data =  who, formula = \"life_exp ~ schooling\").fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd577046",
   "metadata": {},
   "source": [
    "### Linearity\n",
    "There are different ways of checking linearity. In order to put in practice what we've previously learned we will use a scatterplot to check if the relationship between life_exp and schooling is truly linear or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195b3c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatterplot using lmplot from seaborn.\n",
    "sns.lmplot(data = , x = , y = )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0e6ff1",
   "metadata": {},
   "source": [
    "We can say that the scatterplot follows a moderately weak, positive linear associaiton."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ca3e15",
   "metadata": {},
   "source": [
    "### Independence and Equal Variance\n",
    "To check independence and equal variance we will use a scatterplot. More specifically, we will plot the fitted and residual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b66a5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the fitted model to store fitted and residual values\n",
    "\n",
    "# use modelName.predict()\n",
    "fitted = \n",
    "\n",
    "# use modelName.resid()\n",
    "residuals = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e240911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scatterplot using fitted and residual values\n",
    "plt.scatter()\n",
    "\n",
    "# Add horizontal line\n",
    "plt.axhline(y=0, color=\"black\", linestyle=\":\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19316320",
   "metadata": {},
   "source": [
    "Awesome! we can see that the values seem to be randomly scattered around the 0 axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e859e358",
   "metadata": {},
   "source": [
    "### Normality \n",
    "To check normality we will use qqplot, which we imported at the beginning.\n",
    "\n",
    "**Arguments for qqplot:**\n",
    "* data = modelName.resid \n",
    "* line = \"s\" ; standardized line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad78d1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the qqplot. Note that there's a semi-colon at the end, which is to avoid jupyternotebook from returning  \n",
    "qqplot(data = , line = \"s\");\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6899bdf5",
   "metadata": {},
   "source": [
    "The end and beginning of the line seem to fall out of the red line, so we will proceed with caution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aba7b9",
   "metadata": {},
   "source": [
    "Great! our conditions have been satisfied. Let's look at the summary of the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6029f7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89a05a6",
   "metadata": {},
   "source": [
    "Schooling has a positive association of 2.1, with a standard error of 0.034 , with a t-ratio of 60 which gives a p-value < 0.05, corresponding to a 95% confident interval for the beta coefficient between 2.035 and 2.172.\n",
    "\n",
    "We can reject the null hypothesis, since we have enough evidence to say that there's a positive association between schooling and life expectancy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9322cb",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76520d5",
   "metadata": {},
   "source": [
    "Now, what if we consider a third variable?\n",
    "\n",
    "* h0 : there's no difference between country status and country's life expectancy, when adjusting for schooling\n",
    "* ha : there's a difference between country status in country's life expectancy, when adjusting for schooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06826d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we will fit the model. We added a second independent variable, status. As this variable is categorical,\n",
    "# we must add a C before its name.\n",
    "\n",
    "# formula = \"dep_var ~ ind_var1 + C(ind_var2)\"\n",
    "\n",
    "model2 = smf.ols(data=who, formula='life_exp ~ schooling )').fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87743b0",
   "metadata": {},
   "source": [
    "Now, let's check conditions. This is pretty much the same process we did before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ac31d5",
   "metadata": {},
   "source": [
    "### Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5099d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot lmplot with schooling and life_exp. We will add the parameter hue to see how it changes by status.\n",
    "sns.lmplot(data = who, x = \"schooling\", y = \"life_exp\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf8505e",
   "metadata": {},
   "source": [
    "Great! Both groups seem to follow a moderately weak, positive, linear association."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a4aaa2",
   "metadata": {},
   "source": [
    "### Independence and Equal Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5032c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two grids: one for developing and one for developed. \n",
    "g = sns.FacetGrid(data = who, col = , hue = )\n",
    "\n",
    "# Map the residual plots onto the grids.\n",
    "g.map(sns.residplot, \"schooling\", \"life_exp\")\n",
    "\n",
    "# Set labels. See extra material for a more thorough explanation of this step.\n",
    "g.axes[0,0].set_xlabel('residuals')\n",
    "g.axes[0,1].set_xlabel('residuals')\n",
    "g.axes[0,0].set_ylabel('fitted')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ee61e9",
   "metadata": {},
   "source": [
    "Awesome! Both graphs seem to be randomly scattered around the 0 axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75708966",
   "metadata": {},
   "source": [
    "### Normal Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1372366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph qqplot by using model2.resid\n",
    "sma.qqplot(, line = \"s\");\n",
    "\n",
    "# Show graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35b703b",
   "metadata": {},
   "source": [
    "As also seen in the simple linear regression example, the end and beginning of the line seem to fall out of the red line, so we will proceed with caution. Now, let's look at the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0c3afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show model2 summary\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585755c0",
   "metadata": {},
   "source": [
    "Since the p-value < 0.05 we can reject the null hypothesis. Compared to developed countries, countries in development have a lower life expectancy average. In conclusion, there's an association between schooling and life expectency when considering both developed and in development countries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed13211a",
   "metadata": {},
   "source": [
    "Let's predict the life expectancy for a new country. Let's say that this country has an average of 12 years of schooling and it's a developed country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b439fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get values from the table\n",
    "intercept = \n",
    "st_coef = \n",
    "sc_coef =\n",
    "exp = intercept + sc_coef * 12 - st_coef * 0\n",
    "\n",
    "# Print results\n",
    "print(\"The life expenctancy is: \" , exp, \" years.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8697d3f4",
   "metadata": {},
   "source": [
    "We could also create a function to make this process easier and faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125a8e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function and specify our arguments (the values that change)\n",
    "def life_expectancy(schooling , status):\n",
    "    exp = 48.66 + 1.93 * schooling - 3 * status\n",
    "    print(\"The life expectancy is: \" , exp, \" years.\")\n",
    "\n",
    "# Call function\n",
    "life_expectancy(12, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1c36b9",
   "metadata": {},
   "source": [
    "If we recall our scatterplot, we can see that the obtained value makes sense! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3862901",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.lmplot(data = who, x = \"schooling\", y = \"life_exp\", hue = \"Status\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fe3b3c",
   "metadata": {},
   "source": [
    "## Tips\n",
    "* Fit the linear regression model first! You will need it to create some of the graphs.\n",
    "* Assign models and variables to clear and concise names.\n",
    "* Keep your code organized. \n",
    "* Don't forget to import functions and libraries!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a832bc12",
   "metadata": {},
   "source": [
    "## References\n",
    "* https://www.sfu.ca/~mjbrydon/tutorials/BAinPy/09_regression.html\n",
    "* https://research.library.gsu.edu/c.php?g=844869&p=7657842\n",
    "* https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704-ep713_multivariablemethods/bs704-ep713_multivariablemethods2.html\n",
    "+ https://brendanhcullen.github.io/psy611/labs/lab-9.html#data\n",
    "+ https://www.reneshbedre.com/blog/anova.html\n",
    "+ Python Data Analysis - Third Edition. (2019) 2022. Jupyter Notebook. Packt. https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/blob/e1cd8029a1830fe5ecc86379ab361d215e71f036/Chapter05/HR_comma_sep.csv.\n",
    "+ Agresti, Alan, and Maria Kateri. 2021. Foundations of Statistics for Data Scientists: With R and Python. CRC Press.\n",
    "+ De Veaux, Richard D., Paul F. Velleman, and David E. Bock. 2022. Intro Stats. Sixth Edition. Pearson Education, Inc.\n",
    "+ “STAT 2: Modeling with Regression and ANOVA 2E | Student Resources.” n.d. Accessed July 22, 2022. https://www.macmillanlearning.com/studentresources/college/collegebridgepage/stat22e.html.\n",
    "+ Sunel, Khuzema. 2020. “Statistical Inference in Python Using Pandas, NumPy — Part I.” Medium. August 11, 2020. https://towardsdatascience.com/statistical-inference-in-pyhton-using-pandas-numpy-part-i-c2ac0320dffe.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
