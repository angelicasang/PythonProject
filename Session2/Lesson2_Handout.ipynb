{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0223080",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis in Python\n",
    "\n",
    "## Lesson Overview \n",
    "Welcome to lesson two! Today, we will dive into creating, describing, and visualizing dataframes by using the Matplotlib and Seaborn (two Python libraries). Feel free to use our handout to follow along with the video tutorials, and check out the extra material for this lesson in another notebook. Remember that if you have any questions that are not answered by our materials, you are encouraged to get help using the resources that we introduced you to last time. With that being said, let's get started!\n",
    "\n",
    "By the end of this lesson you will be able to:\n",
    "* Create your own dataframes\n",
    "* Load csv files as dataframes\n",
    "* Describe dataframes using Pandas\n",
    "* Visualize dataframes using Pandas, Seaborn and Matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9218fd7",
   "metadata": {},
   "source": [
    "## Importing a dataframe from a csv file\n",
    "\n",
    "We can import a .csv file to work with external data. Importing data is an important skill, seeing as it allows us to work with greater quantities of data. In order to import a .csv file, we need to use the *read_csv( )* funtion from the pandas library. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b> IMPORTANT: </b> The .csv file must be stored in the same folder as this jupyter notebook, or else you will encounter an error because Python cannot find the file you are calling.\n",
    "</div>\n",
    "\n",
    "**Structure:** \n",
    "+ dataset_name = pd.read_csv(\"nameOfFile.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "deccc2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add library\n",
    "import pandas as pd\n",
    "\n",
    "# Read in .csv file\n",
    "penguins = pd.read_csv(\"penguins_wrangled.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6959464",
   "metadata": {},
   "source": [
    "We can look at the first few rows using the *head( )* function, or the last few rows by using the *tail( )* function.\n",
    "\n",
    "**Structure:**\n",
    "+ dataframe.head() = displays the first five rows\n",
    "+ dataframe.tail() = displays the last five rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40483077",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Glimpse the data using head() or tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daa413b",
   "metadata": {},
   "source": [
    "## Univariate Data Exploration\n",
    "\n",
    "In the following section, we will teach you how to use functions in the pandas library to perform univariate data analysis, such as creating frequency tables and calculating descriptive statistics. We will also introduce you to how to describe dataframes that way you can find multiple descriptive statistics after running only one function. Let's get started!\n",
    "\n",
    "### Frequency Table\n",
    "If were interested in seeing a breakdown of all of the different species in this dataset, it would be helpful to create a frequency table. Using the function *value_counts( )* after calling our dataset and variable in function, we can see the count of every unique cell in that column. We will practice creating a frequency table of species using the following structure.\n",
    "\n",
    "**Structure:**\n",
    "+ frequency_table_name = dataset[\"variable_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd619c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign frequency table to a name\n",
    "freq = \n",
    "\n",
    "# Print the table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a962777d",
   "metadata": {},
   "source": [
    "### Relative Frequency Table\n",
    "What if we were interested in finding the relative frequency of a variable in our dataset? In that case, we may want to produce a relative frequency table in which we can see the proportional breakdown of unique categories in our variable. Using the same \"Species\" variable from our penguins dataset, we will follow pretty much the same structure to produce a relative frequency table. By setting the \"normalize\" option within the *value_counts( )* function to **True**, Python knows to return proportions for us instead of just counts.\n",
    "\n",
    "**Structure:**\n",
    "+ frequency_table_name = dataset[\"variable_name\"].value_counts(normalize = **True**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a76565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign relative frequency table to a name\n",
    "# Remember that Python is case-sensitive, so \"true\" must be \"True\" !\n",
    "rel_freq = penguins[\"Species\"].value_counts()\n",
    "\n",
    "# Print the table\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ae41c",
   "metadata": {},
   "source": [
    "### Descriptive Statistics\n",
    "Within the penguins dataset, there are several quantitative variables that we may want to explore. To learn more about the quantitative variables in our dataset, we can calculate sample statistics: mean, variance, and standard deviation. All of these calculations follow the same structure:\n",
    "* mean_all_columns = dataframe_name.mean(numeric_only = True)\n",
    "* median_all_columns = dataframe_name.median(numeric_only = True)\n",
    "* mode_all_columns = dataframe_name.mode(numeric_only = True)\n",
    "* mode_all_columns = dataframe_name.mode(numeric_only = True)\n",
    "* maximum_all_columns = dataframe_name.max(numeric_only = True)\n",
    "* min_all_columns = dataframe_name.min(numeric_only = True)\n",
    "* std_all_columns = dataframe_name.std(numeric_only = True)\n",
    "* var_all_columns = dataframe_name.var(numeric_only = True)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> NOTE: </b> It is possible to skip *numeric_only = True*, however, this will show a warning message. The reason why this is true is because pandas will, by default, calculate these values column by column and some of the columns may include categorical variables. By setting *numeric_only = True*, we are instructing Python to only apply this operation to quantitative variables so it will not throw us an error. If we would instead like to calculate these statistics row by row, we  have to type *axis = 0* as another option inside the parentheses.\n",
    "</div>\n",
    "\n",
    "The code above will calculate the sample statistics for ALL quantitative variables in our dataframe. If we would only like to display one column, we can use the following structure:\n",
    "* mean_column = dataframe_name[\"columnName\"].mean()\n",
    "\n",
    "* median_column = dataframe_name[\"columnName\"].median()\n",
    "\n",
    "* mode_column = dataframe_name[\"columnName\"].mode()\n",
    "\n",
    "* maximum_column = dataframe_name[\"columnName\"].max()\n",
    "\n",
    "* min_column = dataframe_name[\"columnName\"].min()\n",
    "\n",
    "* std_column = dataframe_name[\"columnName\"].std()\n",
    "\n",
    "* var_column = dataframe_name[\"columnName\"].var()\n",
    "\n",
    "Now, let's play around with these new functions using the penguins dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822062e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean across dataframe\n",
    "mean_all = \n",
    "\n",
    "# Display the mean of all columns\n",
    "print(mean_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a913d825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean for the variable \"Culmen Length (mm)\"\n",
    "mean_culmen_length = \n",
    "\n",
    "# Display the mean\n",
    "print(\"Penguins' mean culmen length is equal to: \", mean_culmen_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c583522",
   "metadata": {},
   "source": [
    "If we wanted to calculate range, we can use two of the functions we previously learned: max() and min(). Following the structure above, let's calculate the maximum culmen length and minimum culmen length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59ce423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the max culmen length\n",
    "max_length = \n",
    "\n",
    "# Calculate the min culmen length\n",
    "min_length = \n",
    "\n",
    "# Calculate the range by subtracting min from max\n",
    "length_range = \n",
    "\n",
    "# Print the result\n",
    "print(\"Culmen length range: \", length_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffc384e",
   "metadata": {},
   "source": [
    "Unfortunately, pandas does not have a function that calculates directly the IQR. However, we can find a workaround by calculating Q1 and Q3, and subtracting Q1 from Q3 to produce the IQR. To calculate quantiles and the inter quartile range (aka the IQR), we will use the same structure as before: \n",
    "+ quantile = dataframe[\"columnName\"].quantile()\n",
    "\n",
    "Note: Inside quantile parentheses we have to indicate which quantile we are calculating.\n",
    "* q1 = dataframe[\"columnName\"].quantile(0.25)\n",
    "* q2 = dataframe[\"columnName\"].quantile(0.50)\n",
    "* q3 = dataframe[\"columnName\"].quantile(0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fcfa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the first quantile of \"Body Mass (g)\"\n",
    "q1 = \n",
    "\n",
    "# Calculate the third quantile of \"Body Mass (g)\"\n",
    "q3 = \n",
    "\n",
    "# Calculate the IQR\n",
    "iqr_penguins = \n",
    "print(\"Penguins BM's IQR: \", iqr_penguins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be43d33",
   "metadata": {},
   "source": [
    "### Describing dataframes\n",
    "We already know how to calculate sample statistics one by one, however, if we want to have a table that summarizes these values for *quantitative* variables we can use *describe()*. Using the following structure for this function, we can obtain a data summary that includes counts, mean, standard deviation, minimum, quantiles and maximum. \n",
    "\n",
    "**Structure:**\n",
    "+ dataframe_name.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8079f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e332ed3",
   "metadata": {},
   "source": [
    "However, sometimes we will encounter missing data in our dataframes. To get a sense of how many values are missing we can type *dataframe.isnull( ).sum( )*. As you can see in the example below, there are missing values for the last three variables, so this may impact the values we get when calculating sample statistics for those variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bc923c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8e4b58d",
   "metadata": {},
   "source": [
    "# Data Visualization\n",
    "Now that we have grasped how to summarize our data, let's try plotting it! In this section, we will introduce you to how to create graphs using the python libraries Matplotlib and Seaborn. In our extra material, you can also learn how to plot data using the pandas library. \n",
    "\n",
    "When building data visualizations, our first step is always to import the necessary libraries. You only need to load a library once per Jupyter notebook, but there is no harm in loading a library more than once in a notebook. After loading our libraries, we will need to either import our data or create our data. In this handout, we have already taught you how to import data from a .csv file and previously you learned how to create lists, dictionaries and dataframes. After gathering our data, we will learn how and what functions to use to create and customize our data visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466ac6a6",
   "metadata": {},
   "source": [
    "## Matplotlib and Seaborn\n",
    "Both Matplotlib and Seaborn are quite good for data visualization, however, the Seaborn library can help us to make more advanced statistical plots. Seaborn is an open source Python library and uses Matplotlib as a baselibrary to produce interactive and attractive data visualizations. To begin our data visualization practice, we will construct a line graph using functions only from the Matplotlib library. Gradually, we will add on more functions from both libraries. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994bcf13",
   "metadata": {},
   "source": [
    "### Line Graph\n",
    "Let's begin by importing the Matplotlib library. After, we will use data from the penguins dataset to see a line representation of a single variable, and eventually, a second. The *plt.plot( )* function will do all of the actual plotting for us, and can contain the following options:\n",
    "\n",
    "**Structure for plt.plot():**\n",
    "+ x, y = dataset_name[\"variable_name\"]\n",
    "+ color = desired color of each line\n",
    "+ label = \"name of the plotted variable\"\n",
    "\n",
    "At the end of every new plot that we make, we will run the *plt.show( )* function to display the graph. Let's try it out for the variable \"Culmen Length (mm)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7362cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot data\n",
    "\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7f2f81",
   "metadata": {},
   "source": [
    "Great job! Don't worry if this graph looks kind of confusing right now. With time, we can gain more insight into what this plot is truly telling us. For now, let us continue to add more details to our graph. In the matplotlib module, we can add a title to our graph by writing plt.title() and add labels using plt.xlabel for the x-axis and plt.ylabel for the y-axis. Additionally, we can color our line by assigning color within our plt.plot() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c15731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data and add color\n",
    "plt.plot(penguins[\"Culmen Length (mm)\"], color = \"blue\")\n",
    "\n",
    "# Add label for the x-axis\n",
    "\n",
    "\n",
    "# Add label for the y-axis\n",
    "\n",
    "\n",
    "# Add a title to the graph\n",
    "\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc41b670",
   "metadata": {},
   "source": [
    "What if we wanted to add another line to make a multiple line graph? To accomplish this task we will complete roughly the same process. By adding one more plot command in the same code chunk, we can see line appear on this graph for a different variable. Let's try it out using the !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c962b5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot first set and color it\n",
    "plt.plot(penguins[\"Culmen Length (mm)\"], color = \"blue\")\n",
    "\n",
    "# Plot second set and color it\n",
    "\n",
    "\n",
    "# Add label for the x-axis\n",
    "plt.xlabel(\"Unique Sample Number for a Penguin\")\n",
    "\n",
    "# Add label for the y-axis\n",
    "plt.ylabel(\"Length (mm)\")\n",
    "\n",
    "# Add a title to the graph\n",
    "plt.title(\"Culmen Length and Depth (mm) of Penguins\")\n",
    "\n",
    "# Display the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac00a78",
   "metadata": {},
   "source": [
    "Given that we now have multiple lines on our line graph, it would be helpful to have a legend that distinguishes between the two lines. Luckily, we can use *plt.legend( )* and add another option in our plotting functions to create the legend labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98690cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot first set and color it\n",
    "plt.plot(penguins[\"Culmen Length (mm)\"], color = \"blue\", label = \"Culmen Length\")\n",
    "\n",
    "# Plot second set and color it\n",
    "plt.plot(penguins[\"Culmen Depth (mm)\"], color = \"red\", label = \"Culmen Depth\")\n",
    "\n",
    "# Add label for the x-axis\n",
    "plt.xlabel(\"Unique Sample Number for a Penguin\")\n",
    "\n",
    "# Add label for the y-axis\n",
    "plt.ylabel(\"Length (mm)\")\n",
    "\n",
    "# Add a title to the graph\n",
    "plt.title(\"Culmen Length and Depth (mm) of Penguins\")\n",
    "\n",
    "# Add a legend\n",
    "\n",
    "\n",
    "# Display the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9189b195",
   "metadata": {},
   "source": [
    "After adding all of the details that we did before, our plot is now much more informative and intelligible. We can now tell that we are looking a data visualization of the culmen length and culmen depths of all individual penguins in this dataset. As one might expect, the culmen length is greater than the culmen depth for all penguins. An interesting observation we can make, as well, is that penguins that fall in the latter third of this dataset tend to have smaller culmen depths and larger culmen lengths. In future lessons, we will explore this dataset more and hint at a potential reason why this is true. But, for now, well done! You have just made your first data visualization in Python!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117ade2e",
   "metadata": {},
   "source": [
    "### Bar plot\n",
    "Bar plots are another helpful data visualization that statisticians use to compare the values of various groups; they offer a relationship between a categorical and a continuous variable. As we are plotting a categorical variable and its counts, we will use a seaborn method called **catplot**. A catplot shows the relationship between a numerical and one or more categorical variables.\n",
    "\n",
    "**Arguments**: \n",
    "* data = dataframe\n",
    "* x = \"columnName\"; count how many times each category is repeated\n",
    "* kind = \"graphType\"; the type of graph we want to plot, in this case \"count\"\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Note: </b> We do not need to define y because catplot is counting for us, and assumes that to be the y-axis.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d9c34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add seaborn library\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot the bar plot\n",
    "sns.catplot()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba1aadd",
   "metadata": {},
   "source": [
    "### Histogram plot\n",
    "\n",
    "Histograms are a common way to visualize the distribution of a single numeric variable. They show the probability distribution of a continuous variable, so we will certainly need to use them in our statistical analysis. Let's see how to make histograms using the histplot( ) method from seaborn.\n",
    "\n",
    "**Arguments:**\n",
    "* data = dataframe\n",
    "* x = \"columnName\" ; quantitative variable to display\n",
    "* bins = \"auto\" ; \"auto\" will choose the best bin edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e64515a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add arguments to plot the histogram\n",
    "sns.histplot(data = penguins, x = \"Culmen Length (mm)\", bins = \"auto\")\n",
    "\n",
    "# Label axis\n",
    "plt.xlabel(\"Culmen Length (mm)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Add title\n",
    "plt.title(\"Culmen Length Distribution\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d660158",
   "metadata": {},
   "source": [
    "### Density plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c964841",
   "metadata": {},
   "source": [
    "Density plots will help us visualize how quantitative variables are distributed. In order to graph this, we will use sns.displot. First, we will visualize the density for culmen depth.\n",
    "\n",
    "**Arguments:**\n",
    "* data = dataframe\n",
    "* x = \"columnName\" ; quantitative variable to display\n",
    "* kind = \"graphName\" ; the type of graph we're using; for our purposes \"kde\" (short for kernel distribution estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1f739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add arguments to plot the density plot\n",
    "sns.displot(data = penguins, x = \"Culmen Depth (mm)\", kind = \"kde\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0373ee41",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this section, you have learned how to calculate some basic summary statistics, as well as how to create common plots. They will serve you well as you explore data with Jupyter. To recap, here are the concepts that we covered:\n",
    "\n",
    "Summary statistics:\n",
    "- Mean\n",
    "- Median\n",
    "- Mode\n",
    "- Max, Min, and Range\n",
    "- IQR\n",
    "- Variance\n",
    "- Standard deviation\n",
    "\n",
    "Plots:\n",
    "- Line\n",
    "- Bar\n",
    "- Pie\n",
    "- Histogram\n",
    "- Density"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f850283f",
   "metadata": {},
   "source": [
    "# Tips\n",
    "\n",
    "As you practice the skills learned in this section, please keep in mind the following suggestions:\n",
    "+ Summary functions, by default, will take the whole data frame and calculate by column.\n",
    "+ Always remember to load the necessary libraries.\n",
    "+ Name your variables, dataframes, and labels helpful and informative titles. Avoid using special characters or spaces in names!\n",
    "+ If you get stuck, use the help documentation that you learned in our previous lecture to help you troubleshoot!\n",
    "+ If you want extra material on univariate exploratory data analysis, check out the extra material posted on our repo!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac666967",
   "metadata": {},
   "source": [
    "# References\n",
    "Main resources that we used to build this material:\n",
    "\n",
    "* Navlani, Avinash, Armando Fandango, and Ivan Idris. 2021. Python Data Analysis. Third Edition. Birmingham: Packt Publishing.\n",
    "\n",
    "* Python Data Analysis - Third Edition. (2019). Jupyter Notebook. Packt. https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/blob/e1cd8029a1830fe5ecc86379ab361d215e71f036/Chapter05/HR_comma_sep.csv. Accessed July 7, 2022.\n",
    "\n",
    "* Numpy documentation: https://numpy.org/doc/\n",
    "\n",
    "* Pandas documentation: https://pandas.pydata.org/docs/\n",
    "\n",
    "* Seaborn documentation: https://seaborn.pydata.org/tutorial.html \n",
    "\n",
    "* Statistics documentation: https://docs.python.org/3/library/statistics.html\n",
    "\n",
    "Although the Python documentation is usually comprehensive, these websites can be simpler, more user-friendly alternatives:\n",
    "\n",
    "* GeeksforGeeks: https://www.geeksforgeeks.org/python-programming-language/\n",
    "\n",
    "* W3schools: https://www.w3schools.com/python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
